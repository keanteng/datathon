{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the library for path\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import openpyxl as xl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data set\n",
    "data_dir = Path('../datathon')\n",
    "\n",
    "# read xlsx data set\n",
    "df = pd.read_csv(data_dir/'VacancyRate.csv', index_col = [0], parse_dates=[0])\n",
    "\n",
    "# remove all the empty column\n",
    "df = df.dropna(axis=1, how='all')\n",
    "df.dropna(inplace=True)\n",
    "df.index.format = '%Y-%m-%d'\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df)\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Vacancy Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test for Data Set\n",
    "## 3.1 Stationary Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[1]):\n",
    "    column_name = df.columns[i]\n",
    "    column_data = df.iloc[:, i]  # Access column data using iloc\n",
    "\n",
    "    # Perform the ADF test\n",
    "    adft = adfuller(column_data, autolag='AIC')\n",
    "    \n",
    "    if df.iloc[:, i].isna().any():\n",
    "        df = df.dropna()\n",
    "\n",
    "    # Create a DataFrame to display the ADF test results\n",
    "    output_df = pd.DataFrame({\n",
    "        \"Values\": [adft[0], adft[1], adft[2], adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']],\n",
    "        \"Metric\": [\"Test Statistics\", \"p-value\", \"No. of lags used\", \"Number of observations used\",\n",
    "                   \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]\n",
    "    })\n",
    "\n",
    "    # Print the column name and ADF test results\n",
    "    print(f\"Results for {column_name}:\")\n",
    "    print(output_df, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[1]):\n",
    "    column_name = df.columns[i]\n",
    "    print(f\"Results for {column_name}:\")\n",
    "\n",
    "    autocorrelation_lag1 = df.iloc[:, i].autocorr(lag=1)\n",
    "    print(\"One Month Lag: \", autocorrelation_lag1)\n",
    "\n",
    "    autocorrelation_lag3 = df.iloc[:, i].autocorr(lag=3)\n",
    "    print(\"Three Month Lag: \", autocorrelation_lag3)\n",
    "\n",
    "    autocorrelation_lag6 = df.iloc[:, i].autocorr(lag=6)\n",
    "    print(\"Six Month Lag: \", autocorrelation_lag6)\n",
    "\n",
    "    autocorrelation_lag9 = df.iloc[:, i].autocorr(lag=9)\n",
    "    print(\"Nine Month Lag: \", autocorrelation_lag9)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    column_name = df.columns[i]\n",
    "    fig, ax = plt.subplots(2, figsize=(12,6))\n",
    "    ax[0] = plot_acf(df.iloc[:, i], ax=ax[0], lags=6, title=f\"Autocorrelation for {column_name}\")\n",
    "    ax[1] = plot_pacf(df.iloc[:, i], ax=ax[1], lags=6, title=f\"Partial Autocorrelation for {column_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = df.diff().dropna()\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    column_name = df.columns[i]\n",
    "    print(f\"Results for {column_name}:\")\n",
    "    print(\"After differencing:\")\n",
    "\n",
    "    # perform the stationarity test\n",
    "    adft = adfuller(df_diff.iloc[:, i], autolag='AIC')\n",
    "    output_df = pd.DataFrame({\"Values\":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']]  , \"Metric\":[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\", \n",
    "                                                        \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]})\n",
    "    print(output_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # autocorrelation\n",
    "    from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "    fig, ax = plt.subplots(2, figsize=(12,6))\n",
    "    ax[0] = plot_acf(df_diff.iloc[:,i], ax=ax[0], lags=6, title= f\"Autocorrelation for {column_name}\")\n",
    "    ax[1] = plot_pacf(df_diff.iloc[:,i], ax=ax[1], lags=6, title= f\"Partial Autocorrelation for {column_name}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df_diff to data frame\n",
    "df_diff = pd.DataFrame(df_diff)\n",
    "df_diff.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second order differencing\n",
    "df_diff_diff = df_diff['Pulau Pinang'].diff().dropna()\n",
    "df_diff_diff = df_diff['W.P Kuala Lumpur'].diff().dropna()\n",
    "df_diff_diff = df_diff['Pulau Pinang'].diff().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.iloc[:round(len(df)*0.8)]\n",
    "test = df.iloc[round(len(df)*0.8):]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ETS(train,test,column_number):\n",
    "    Trend_choice = ['add','mul']\n",
    "    Seasonal_choice = ['add','mul']\n",
    "\n",
    "    MSE = pd.DataFrame({'Trend': [], 'Seasonal': [], 'MSE': [], 'RMSE': [], 'MAE': []})\n",
    "\n",
    "    for value in Trend_choice:\n",
    "        for value2 in Seasonal_choice:\n",
    "            model = ExponentialSmoothing(train.iloc[:, column_number], trend=value, seasonal=value2, seasonal_periods=6)  \n",
    "            model_fit = model.fit()\n",
    "            forecasts = model_fit.forecast(steps=3)\n",
    "            # Check the error measure for each model and update the MSE_johor DataFrame\n",
    "            new_row = pd.DataFrame({'Trend': value, \n",
    "                                    'Seasonal': value2, \n",
    "                                    'MSE': mean_squared_error(test.iloc[:, column_number], forecasts),\n",
    "                                    'RMSE': np.sqrt(mean_squared_error(test.iloc[:, column_number], forecasts)),\n",
    "                                    'MAE': mean_absolute_error(test.iloc[:, column_number], forecasts)}, index=[0]) \n",
    "            MSE = pd.concat([MSE, new_row], ignore_index=True)\n",
    "    return MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETS_johor = find_ETS(train,test,0)\n",
    "ETS_kedah = find_ETS(train,test,1)\n",
    "ETS_kelantan = find_ETS(train,test,2)\n",
    "ETS_melaka = find_ETS(train,test,3)\n",
    "ETS_n9 = find_ETS(train,test,4)\n",
    "ETS_pahang = find_ETS(train,test,5)\n",
    "ETS_penang = find_ETS(train,test,6)\n",
    "ETS_perak = find_ETS(train,test,7)\n",
    "ETS_perlis = find_ETS(train,test,8)\n",
    "ETS_selangor = find_ETS(train,test,9)\n",
    "ETS_terengganu = find_ETS(train,test,10)\n",
    "ETS_sabah = find_ETS(train,test,11)\n",
    "ETS_sarawak = find_ETS(train,test,12)\n",
    "ETS_wpkl = find_ETS(train,test,13)\n",
    "ETS_wplabuan = find_ETS(train,test,14)\n",
    "ETS_p = find_ETS(train,test,15)\n",
    "ETS_Total = find_ETS(train,test,16)\n",
    "\n",
    "# sort the output above by MSE ascending\n",
    "ETS_johor.sort_values(by=['MSE'])\n",
    "ETS_kedah.sort_values(by=['MSE'])\n",
    "ETS_kelantan.sort_values(by=['MSE'])\n",
    "ETS_melaka.sort_values(by=['MSE'])\n",
    "ETS_n9.sort_values(by=['MSE'])\n",
    "ETS_pahang.sort_values(by=['MSE'])\n",
    "ETS_penang.sort_values(by=['MSE'])\n",
    "ETS_perak.sort_values(by=['MSE'])\n",
    "ETS_perlis.sort_values(by=['MSE'])\n",
    "ETS_selangor.sort_values(by=['MSE'])\n",
    "ETS_terengganu.sort_values(by=['MSE'])\n",
    "ETS_sabah.sort_values(by=['MSE'])\n",
    "ETS_sarawak.sort_values(by=['MSE'])\n",
    "ETS_wpkl.sort_values(by=['MSE'])\n",
    "ETS_wplabuan.sort_values(by=['MSE'])\n",
    "ETS_p.sort_values(by=['MSE'])\n",
    "ETS_Total.sort_values(by=['MSE'])\n",
    "\n",
    "# combine first row of output above into one dataframe\n",
    "ETS = pd.concat([ETS_johor.iloc[0], ETS_kedah.iloc[0], ETS_kelantan.iloc[0], ETS_melaka.iloc[0], ETS_n9.iloc[0], ETS_pahang.iloc[0], ETS_penang.iloc[0], ETS_perak.iloc[0], ETS_perlis.iloc[0], ETS_selangor.iloc[0], ETS_terengganu.iloc[0], ETS_sabah.iloc[0], ETS_sarawak.iloc[0], ETS_wpkl.iloc[0], ETS_wplabuan.iloc[0], ETS_p.iloc[0],ETS_Total.iloc[0]], axis=1)\n",
    "ETS = ETS.T\n",
    "ETS.index = train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARIMA_forecast = []\n",
    "ARIMA_forecast = pd.DataFrame(ARIMA_forecast)\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    column_name = df.columns[i]\n",
    "    print(f\"Results for {column_name}:\")\n",
    "\n",
    "    ARIMAmodel = auto_arima(train[[column_name]], trace=True, error_action='ignore', suppress_warnings=True)\n",
    "    ARIMAmodel.fit(train[[column_name]])\n",
    "    forecast = ARIMAmodel.predict(n_periods=len(test))\n",
    "    ARIMA_forecast = pd.concat([ARIMA_forecast, forecast], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARIMA_forecast.index = ['2022-01-07', '2022-01-10', '2023-01-01']\n",
    "ARIMA_forecast.index = pd.to_datetime(ARIMA_forecast.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[1]):\n",
    "    column_name = df.columns[i]\n",
    "    column_data = df.iloc[:, i]  # Access column data using iloc\n",
    "    \n",
    "    # plot the predictions for validation set\n",
    "    # set the starting date at 2019-01-01\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(train.iloc[:,i], label='Train')\n",
    "    plt.plot(test.iloc[:,i], label='test data')\n",
    "    plt.plot(ARIMA_forecast.iloc[:,i], label='Prediction')\n",
    "    plt.title(f'ARIMA Forecast for {column_name}')\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA Accuracy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a empty data frame to store the error\n",
    "error_df_ARIMA = pd.DataFrame({'MSE': [], 'RMSE': [], 'MAE': []})\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    mae = mean_absolute_error(test.iloc[:,i], ARIMA_forecast.iloc[:,i])\n",
    "    mse = mean_squared_error(test.iloc[:,i], ARIMA_forecast.iloc[:,i])\n",
    "    rmse = np.sqrt(mse)\n",
    "    # Create a DataFrame from the current values\n",
    "    new_row = pd.DataFrame({'MSE': [mse], 'RMSE': [rmse], 'MAE': [mae]})\n",
    "\n",
    "    # Concatenate the new DataFrame with the existing error_df_ARIMA\n",
    "    error_df_ARIMA = pd.concat([error_df_ARIMA, new_row], ignore_index=True)\n",
    "\n",
    "error_df_ARIMA.index = test.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare all the error from error_df_ETS and error_df_ARIMA by row \n",
    "ETS['Model'] = 'ETS'\n",
    "error_df_ARIMA['Model'] = 'ARIMA'\n",
    "print(ETS)\n",
    "print(error_df_ARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Johor - ARIMA\n",
    "# Kedah - ETS\n",
    "# Kelantan - ARIMA\n",
    "# Melaka - ETS\n",
    "# N9 - ETS\n",
    "# Pahang - ETS\n",
    "# Pulau Pinang - ARIMA\n",
    "# Perak - ARIMA\n",
    "# Perlis - ETS\n",
    "# Selangor - ARIMA\n",
    "# Terengganu - ETS\n",
    "# Sabah - ETS\n",
    "# Sarawak - ETS\n",
    "# Kuala Lumpur - ARIMA\n",
    "# Labuan - ARIMA\n",
    "# Putrajaya - ETS\n",
    "# Total - ARIMA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
