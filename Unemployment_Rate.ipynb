{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the library for path\n",
    "from pathlib import Path\n",
    "import openpyxl as xl\n",
    "import numpy as np\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from numpy import *\n",
    "from datetime import datetime\n",
    "from statsmodels import *\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
    "from pmdarima.utils import diff_inv\n",
    "from statsmodels import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data set\n",
    "data_dir = Path('../datathon')\n",
    "\n",
    "# read xlsx data set\n",
    "df = pd.read_csv(data_dir/'Unemployment Rate.csv', index_col = [0], parse_dates=[0])\n",
    "\n",
    "# remove all the empty column\n",
    "df = df.dropna(axis=1, how='all')\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test for Data Set\n",
    "## 3.1 Stationary Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[1]):\n",
    "    column_name = df.columns[i]\n",
    "    column_data = df.iloc[:, i]  # Access column data using iloc\n",
    "\n",
    "    # Perform the ADF test\n",
    "    adft = adfuller(column_data, autolag='AIC')\n",
    "    \n",
    "    if df.iloc[:, i].isna().any():\n",
    "        df = df.dropna()\n",
    "\n",
    "    # Create a DataFrame to display the ADF test results\n",
    "    output_df = pd.DataFrame({\n",
    "        \"Values\": [adft[0], adft[1], adft[2], adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']],\n",
    "        \"Metric\": [\"Test Statistics\", \"p-value\", \"No. of lags used\", \"Number of observations used\",\n",
    "                   \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]\n",
    "    })\n",
    "\n",
    "    # Print the column name and ADF test results\n",
    "    print(f\"Results for {column_name}:\")\n",
    "    print(output_df, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[1]):\n",
    "    column_name = df.columns[i]\n",
    "    print(f\"Results for {column_name}:\")\n",
    "\n",
    "    autocorrelation_lag1 = df.iloc[:, i].autocorr(lag=1)\n",
    "    print(\"One Month Lag: \", autocorrelation_lag1)\n",
    "\n",
    "    autocorrelation_lag3 = df.iloc[:, i].autocorr(lag=3)\n",
    "    print(\"Three Month Lag: \", autocorrelation_lag3)\n",
    "\n",
    "    autocorrelation_lag6 = df.iloc[:, i].autocorr(lag=6)\n",
    "    print(\"Six Month Lag: \", autocorrelation_lag6)\n",
    "\n",
    "    autocorrelation_lag9 = df.iloc[:, i].autocorr(lag=9)\n",
    "    print(\"Nine Month Lag: \", autocorrelation_lag9)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    column_name = df.columns[i]\n",
    "    fig, ax = plt.subplots(2, figsize=(12,6))\n",
    "    ax[0] = plot_acf(df.iloc[:, i], ax=ax[0], lags=6, title=f\"Autocorrelation for {column_name}\")\n",
    "    ax[1] = plot_pacf(df.iloc[:, i], ax=ax[1], lags=6, title=f\"Partial Autocorrelation for {column_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = df.diff().dropna()\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    column_name = df.columns[i]\n",
    "    print(f\"Results for {column_name}:\")\n",
    "    print(\"After differencing:\")\n",
    "\n",
    "    # perform the stationarity test\n",
    "    adft = adfuller(df_diff.iloc[:, i], autolag='AIC')\n",
    "    output_df = pd.DataFrame({\"Values\":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']]  , \"Metric\":[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\", \n",
    "                                                        \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]})\n",
    "    print(output_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # autocorrelation\n",
    "    from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "    fig, ax = plt.subplots(2, figsize=(12,6))\n",
    "    ax[0] = plot_acf(df_diff.iloc[:,i], ax=ax[0], lags=6, title= f\"Autocorrelation for {column_name}\")\n",
    "    ax[1] = plot_pacf(df_diff.iloc[:,i], ax=ax[1], lags=6, title= f\"Partial Autocorrelation for {column_name}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df_diff to data frame\n",
    "df_diff = pd.DataFrame(df_diff)\n",
    "df_diff.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second order differencing\n",
    "df_diff_diff = df_diff['Pulau Pinang'].diff().dropna()\n",
    "df_diff_diff = df_diff['W.P Kuala Lumpur'].diff().dropna()\n",
    "df_diff_diff = df_diff['Pulau Pinang'].diff().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.iloc[:round(len(df)*0.8)]\n",
    "test = df.iloc[round(len(df)*0.8):]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ETS(train,test,column_number):\n",
    "    Trend_choice = ['add','mul']\n",
    "    Seasonal_choice = ['add','mul']\n",
    "\n",
    "    MSE = pd.DataFrame({'Trend': [], 'Seasonal': [], 'MSE': [], 'RMSE': [], 'MAE': []})\n",
    "\n",
    "    for value in Trend_choice:\n",
    "        for value2 in Seasonal_choice:\n",
    "            model = ExponentialSmoothing(train.iloc[:, column_number], trend=value, seasonal=value2, seasonal_periods=6)  \n",
    "            model_fit = model.fit()\n",
    "            forecasts = model_fit.forecast(steps=4)\n",
    "            # Check the error measure for each model and update the MSE_johor DataFrame\n",
    "            new_row = pd.DataFrame({'Trend': value, \n",
    "                                    'Seasonal': value2, \n",
    "                                    'MSE': mean_squared_error(test.iloc[:, column_number], forecasts),\n",
    "                                    'RMSE': np.sqrt(mean_squared_error(test.iloc[:, column_number], forecasts)),\n",
    "                                    'MAE': mean_absolute_error(test.iloc[:, column_number], forecasts)}, index=[0]) \n",
    "            MSE = pd.concat([MSE, new_row], ignore_index=True)\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETS_johor = find_ETS(train,test,0)\n",
    "ETS_kedah = find_ETS(train,test,1)\n",
    "ETS_kelantan = find_ETS(train,test,2)\n",
    "ETS_melaka = find_ETS(train,test,3)\n",
    "ETS_n9 = find_ETS(train,test,4)\n",
    "ETS_pahang = find_ETS(train,test,5)\n",
    "ETS_penang = find_ETS(train,test,6)\n",
    "ETS_perak = find_ETS(train,test,7)\n",
    "ETS_perlis = find_ETS(train,test,8)\n",
    "ETS_selangor = find_ETS(train,test,9)\n",
    "ETS_terengganu = find_ETS(train,test,10)\n",
    "ETS_sabah = find_ETS(train,test,11)\n",
    "ETS_sarawak = find_ETS(train,test,12)\n",
    "ETS_wpkl = find_ETS(train,test,13)\n",
    "ETS_wplabuan = find_ETS(train,test,14)\n",
    "ETS_p = find_ETS(train,test,15)\n",
    "ETS_Total = find_ETS(train,test,16)\n",
    "\n",
    "# sort the output above by MSE ascending\n",
    "ETS_johor = ETS_johor.sort_values(by=['MSE'])\n",
    "ETS_kedah = ETS_kedah.sort_values(by=['MSE'])\n",
    "ETS_kelantan = ETS_kelantan.sort_values(by=['MSE'])\n",
    "ETS_melaka = ETS_melaka.sort_values(by=['MSE'])\n",
    "ETS_n9 = ETS_n9.sort_values(by=['MSE'])\n",
    "ETS_pahang = ETS_pahang.sort_values(by=['MSE'])\n",
    "ETS_penang = ETS_penang.sort_values(by=['MSE'])\n",
    "ETS_perak = ETS_perak.sort_values(by=['MSE'])\n",
    "ETS_perlis = ETS_perlis.sort_values(by=['MSE'])\n",
    "ETS_selangor = ETS_selangor.sort_values(by=['MSE'])\n",
    "ETS_terengganu = ETS_terengganu.sort_values(by=['MSE'])\n",
    "ETS_sabah = ETS_sabah = ETS_sabah.sort_values(by=['MSE'])\n",
    "ETS_sarawak = ETS_sarawak.sort_values(by=['MSE'])\n",
    "ETS_wpkl = ETS_wpkl.sort_values(by=['MSE'])\n",
    "ETS_wplabuan = ETS_wplabuan.sort_values(by=['MSE'])\n",
    "ETS_p = ETS_p.sort_values(by=['MSE'])\n",
    "ETS_Total = ETS_Total.sort_values(by=['MSE'])\n",
    "\n",
    "# combine first row of output above into one dataframe\n",
    "ETS = pd.concat([ETS_johor.iloc[0], ETS_kedah.iloc[0], ETS_kelantan.iloc[0], ETS_melaka.iloc[0], ETS_n9.iloc[0], ETS_pahang.iloc[0], ETS_penang.iloc[0], ETS_perak.iloc[0], ETS_perlis.iloc[0], ETS_selangor.iloc[0], ETS_terengganu.iloc[0], ETS_sabah.iloc[0], ETS_sarawak.iloc[0], ETS_wpkl.iloc[0], ETS_wplabuan.iloc[0], ETS_p.iloc[0],ETS_Total.iloc[0]], axis=1)\n",
    "ETS = ETS.T\n",
    "ETS.index = train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARIMA_forecast = []\n",
    "ARIMA_forecast = pd.DataFrame(ARIMA_forecast)\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    column_name = df.columns[i]\n",
    "    print(f\"Results for {column_name}:\")\n",
    "\n",
    "    ARIMAmodel = auto_arima(train[[column_name]], trace=True, error_action='ignore', suppress_warnings=True)\n",
    "    ARIMAmodel.fit(train[[column_name]])\n",
    "    forecast = ARIMAmodel.predict(n_periods=len(test))\n",
    "    ARIMA_forecast = pd.concat([ARIMA_forecast, forecast], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARIMA_forecast.index = ['2022-01-07', '2022-01-10', '2023-01-01', '2023-01-04']\n",
    "ARIMA_forecast.index = pd.to_datetime(ARIMA_forecast.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[1]):\n",
    "    column_name = df.columns[i]\n",
    "    column_data = df.iloc[:, i]  # Access column data using iloc\n",
    "    \n",
    "    # plot the predictions for validation set\n",
    "    # set the starting date at 2019-01-01\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(train.iloc[:,i], label='Train')\n",
    "    plt.plot(test.iloc[:,i], label='test data')\n",
    "    plt.plot(ARIMA_forecast.iloc[:,i], label='Prediction')\n",
    "    plt.title(f'ARIMA Forecast for {column_name}')\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA Accuracy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a empty data frame to store the error\n",
    "error_df_ARIMA = pd.DataFrame({'MSE': [], 'RMSE': [], 'MAE': []})\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    mae = mean_absolute_error(test.iloc[:,i], ARIMA_forecast.iloc[:,i])\n",
    "    mse = mean_squared_error(test.iloc[:,i], ARIMA_forecast.iloc[:,i])\n",
    "    rmse = np.sqrt(mse)\n",
    "    # Create a DataFrame from the current values\n",
    "    new_row = pd.DataFrame({'MSE': [mse], 'RMSE': [rmse], 'MAE': [mae]})\n",
    "\n",
    "    # Concatenate the new DataFrame with the existing error_df_ARIMA\n",
    "    error_df_ARIMA = pd.concat([error_df_ARIMA, new_row], ignore_index=True)\n",
    "\n",
    "error_df_ARIMA.index = test.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Trend Seasonal       MSE      RMSE       MAE Model\n",
      "Johor              mul      mul   0.18588  0.431138  0.408426   ETS\n",
      "Kedah              mul      mul  0.064746  0.254451  0.199174   ETS\n",
      "Kelantan           mul      mul   0.08277  0.287697  0.244506   ETS\n",
      "Melaka             add      mul  0.260779  0.510666  0.461981   ETS\n",
      "Negeri Sembilan    add      mul  0.244852  0.494825  0.461495   ETS\n",
      "Pahang             mul      add  0.150803  0.388333  0.349813   ETS\n",
      "Pulau Pinang       mul      add  0.078689  0.280515  0.258557   ETS\n",
      "Perak              mul      add  0.294963  0.543105  0.477087   ETS\n",
      "Perlis             mul      add  0.288382  0.537012  0.444601   ETS\n",
      "Selangor           mul      mul  0.559253  0.747832  0.736028   ETS\n",
      "Terengganu         add      mul  0.097659  0.312505  0.233489   ETS\n",
      "Sabah              add      add  0.057087  0.238929  0.191914   ETS\n",
      "Sarawak            mul      mul  0.390166  0.624633  0.542165   ETS\n",
      "W.P Kuala Lumpur   mul      mul  0.341967   0.58478  0.504253   ETS\n",
      "W.P Labuan         mul      mul   0.45562  0.674996  0.558881   ETS\n",
      "W.P Putrajaya      add      add  2.297556  1.515769  1.116474   ETS\n",
      "Total              mul      mul  0.248375  0.498373  0.430752   ETS\n",
      "                       MSE      RMSE       MAE  Model\n",
      "Johor             0.262908  0.512746  0.471429  ARIMA\n",
      "Kedah             0.538690  0.733955  0.724763  ARIMA\n",
      "Kelantan          0.284726  0.533598  0.456257  ARIMA\n",
      "Melaka            0.664942  0.815440  0.803851  ARIMA\n",
      "Negeri Sembilan   0.077623  0.278609  0.218363  ARIMA\n",
      "Pahang            0.495718  0.704072  0.660744  ARIMA\n",
      "Pulau Pinang      0.077839  0.278997  0.235777  ARIMA\n",
      "Perak             0.350469  0.592004  0.565324  ARIMA\n",
      "Perlis            1.897908  1.377646  1.367857  ARIMA\n",
      "Selangor          2.822333  1.679980  1.653810  ARIMA\n",
      "Terengganu        0.683382  0.826669  0.679743  ARIMA\n",
      "Sabah             0.064016  0.253014  0.199877  ARIMA\n",
      "Sarawak           0.607500  0.779423  0.725000  ARIMA\n",
      "W.P Kuala Lumpur  0.438622  0.662286  0.617857  ARIMA\n",
      "W.P Labuan        0.967500  0.983616  0.975000  ARIMA\n",
      "W.P Putrajaya     1.664553  1.290175  1.045734  ARIMA\n",
      "Total             0.141762  0.376513  0.364209  ARIMA\n"
     ]
    }
   ],
   "source": [
    "# compare all the error from error_df_ETS and error_df_ARIMA by row \n",
    "ETS['Model'] = 'ETS'\n",
    "error_df_ARIMA['Model'] = 'ARIMA'\n",
    "print(ETS)\n",
    "print(error_df_ARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Johor - ETS\n",
    "# Kedah - ETS\n",
    "# Kelantan - ETS\n",
    "# Melaka - ETS\n",
    "# N9 - ARIMA\n",
    "# Pahang - ETS\n",
    "# Pulau Pinang - ARIMA\n",
    "# Perak - ETS\n",
    "# Perlis - ETS\n",
    "# Selangor - ETS\n",
    "# Terengganu - ETS\n",
    "# Sabah - ETS\n",
    "# Sarawak - ETS\n",
    "# Kuala Lumpur - ARIMA\n",
    "# Labuan - ETS\n",
    "# Putrajaya - ARIMA\n",
    "# Total - ARIMA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
